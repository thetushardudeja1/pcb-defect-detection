{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2890d71e",
   "metadata": {},
   "source": [
    "## Feature Extraction using ResNet-18\n",
    "\n",
    "In this notebook, a pretrained ResNet-18 model is used as a feature extractor for PCB defect images. The convolutional layers pretrained on ImageNet are reused to extract discriminative visual features, while the final classification layer is removed. All feature extraction layers are frozen to prevent weight updates during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22fefc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# basic imports and setup\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms as T\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# makes CUDA errors easier to trace\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# dataset location\n",
    "RAW_DATA_ROOT = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\data\\raw\\DeepPCB\"\n",
    ")\n",
    "\n",
    "# train / test split files\n",
    "train_split = RAW_DATA_ROOT / \"trainval.txt\"\n",
    "test_split  = RAW_DATA_ROOT / \"test.txt\"\n",
    "\n",
    "# training params\n",
    "IMAGE_SIZE   = 224\n",
    "BATCH_SIZE   = 32\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = False\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_EPOCHS   = 2\n",
    "\n",
    "# pick device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "721b4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transforms for training and validation\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# used during training\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# used for validation / testing\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# dataset for DeepPCB patch-based data\n",
    "class DeepPCBPatchDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split_file, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # split file has image path and annotation path per line\n",
    "        with open(split_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "\n",
    "                img_rel, ann_rel = parts\n",
    "                img_path = self.root_dir / img_rel\n",
    "                ann_path = self.root_dir / ann_rel\n",
    "\n",
    "                # handle renamed images if present\n",
    "                if not img_path.exists():\n",
    "                    alt_img_path = img_path.with_name(\n",
    "                        img_path.stem + \"_temp\" + img_path.suffix\n",
    "                    )\n",
    "                    if alt_img_path.exists():\n",
    "                        img_path = alt_img_path\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if not ann_path.exists():\n",
    "                    continue\n",
    "\n",
    "                # read bounding boxes from annotation file\n",
    "                with open(ann_path, \"r\") as af:\n",
    "                    for line in af:\n",
    "                        x_min, y_min, x_max, y_max, cls_id = map(\n",
    "                            int, line.strip().split()\n",
    "                        )\n",
    "\n",
    "                        # store one patch per bounding box\n",
    "                        self.samples.append(\n",
    "                            (img_path, (x_min, y_min, x_max, y_max), cls_id - 1)\n",
    "                        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, bbox, cls_id = self.samples[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # crop defect area\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        patch = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "\n",
    "        return patch.float(), torch.tensor(cls_id, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e669a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6873\n",
      "Validation samples: 3140\n",
      "Batch shape: torch.Size([128, 3, 224, 224])\n",
      "Label type: torch.int64\n",
      "Unique labels: tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# dataset instantiation and basic checks\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# create training dataset\n",
    "train_dataset = DeepPCBPatchDataset(\n",
    "    root_dir=RAW_DATA_ROOT,\n",
    "    split_file=train_split,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# create validation dataset\n",
    "val_dataset = DeepPCBPatchDataset(\n",
    "    root_dir=RAW_DATA_ROOT,\n",
    "    split_file=test_split,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "\n",
    "# dataloaders\n",
    "BATCH_SIZE = 128  # larger batch since patches are small\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "# quick sanity check on one batch\n",
    "imgs, labels = next(iter(train_loader))\n",
    "print(\"Batch shape:\", imgs.shape)\n",
    "print(\"Label type:\", labels.dtype)\n",
    "print(\"Unique labels:\", labels.unique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f3cf7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (3140, 512)\n",
      "Labels shape: (3140,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward through ResNet feature blocks\n",
    "        x = model.conv1(imgs)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "\n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "\n",
    "        # global average pooling\n",
    "        x = model.avgpool(x)\n",
    "\n",
    "        # flatten to (batch_size, 512)\n",
    "        features = torch.flatten(x, 1)\n",
    "\n",
    "        all_features.append(features.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# stack all batches\n",
    "all_features = np.vstack(all_features)\n",
    "all_labels = np.hstack(all_labels)\n",
    "\n",
    "print(\"Feature matrix shape:\", all_features.shape)\n",
    "print(\"Labels shape:\", all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "046daf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature files:\n",
      "C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\features\\val_features.npy\n",
      "C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\features\\val_labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# directory to store extracted features\n",
    "FEATURES_DIR = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\features\"\n",
    ")\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save validation features and labels\n",
    "np.save(FEATURES_DIR / \"val_features.npy\", all_features)\n",
    "np.save(FEATURES_DIR / \"val_labels.npy\", all_labels)\n",
    "\n",
    "print(\"Saved feature files:\")\n",
    "print(FEATURES_DIR / \"val_features.npy\")\n",
    "print(FEATURES_DIR / \"val_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3990acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature matrix shape: (6873, 512)\n",
      "Training labels shape: (6873,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward through ResNet feature layers\n",
    "        x = model.conv1(imgs)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "\n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "\n",
    "        # global average pooling\n",
    "        x = model.avgpool(x)\n",
    "\n",
    "        # flatten to (batch_size, 512)\n",
    "        features = torch.flatten(x, 1)\n",
    "\n",
    "        train_features.append(features.cpu().numpy())\n",
    "        train_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# stack all batches\n",
    "train_features = np.vstack(train_features)\n",
    "train_labels = np.hstack(train_labels)\n",
    "\n",
    "print(\"Training feature matrix shape:\", train_features.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b33a53ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "FEATURES_DIR = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\features\"\n",
    ")\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(FEATURES_DIR / \"train_features.npy\", train_features)\n",
    "np.save(FEATURES_DIR / \"train_labels.npy\", train_labels)\n",
    "\n",
    "np.save(FEATURES_DIR / \"val_features.npy\", all_features)\n",
    "np.save(FEATURES_DIR / \"val_labels.npy\", all_labels)\n",
    "\n",
    "print(\"All features and labels saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35c2a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (6873, 512)\n",
      "Train labels shape: (6873,)\n",
      "Val features shape: (3140, 512)\n",
      "Val labels shape: (3140,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "FEATURES_DIR = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\features\"\n",
    ")\n",
    "\n",
    "X_train = np.load(FEATURES_DIR / \"train_features.npy\")\n",
    "y_train = np.load(FEATURES_DIR / \"train_labels.npy\")\n",
    "\n",
    "X_val = np.load(FEATURES_DIR / \"val_features.npy\")\n",
    "y_val = np.load(FEATURES_DIR / \"val_labels.npy\")\n",
    "\n",
    "print(\"Train features shape:\", X_train.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "print(\"Val features shape:\", X_val.shape)\n",
    "print(\"Val labels shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39be768a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.3\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35a58428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=600,              # fixed number of trees\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    gamma=0.1,\n",
    "    min_child_weight=3,\n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=1.0,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=6,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97990777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9394904458598726\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9394    0.9408    0.9401       659\n",
      "           1     0.9716    0.9310    0.9509       478\n",
      "           2     0.8679    0.9420    0.9034       586\n",
      "           3     0.9131    0.8489    0.8798       483\n",
      "           4     0.9872    0.9957    0.9914       464\n",
      "           5     0.9829    0.9809    0.9819       470\n",
      "\n",
      "    accuracy                         0.9395      3140\n",
      "   macro avg     0.9437    0.9399    0.9413      3140\n",
      "weighted avg     0.9405    0.9395    0.9395      3140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_prob = xgb_model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", acc)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d02f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid metadata saved at: C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\\hybrid_model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# define model directory again (safe and correct)\n",
    "MODEL_DIR = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\"\n",
    ")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = {\n",
    "    \"model_type\": \"CNN + XGBoost Hybrid\",\n",
    "    \"cnn_backbone\": \"ResNet-18\",\n",
    "    \"embedding_dim\": 512,\n",
    "    \"num_classes\": 6,\n",
    "    \"patch_level_accuracy\": 0.9417,\n",
    "    \"cnn_accuracy\": 0.93,\n",
    "    \"dataset\": \"DeepPCB (patch-based)\",\n",
    "    \"notes\": \"Hybrid improves macro F1 and robustness over CNN alone\"\n",
    "}\n",
    "\n",
    "meta_path = MODEL_DIR / \"hybrid_model_metadata.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"Hybrid metadata saved at: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3a872d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to directory:\n",
      "C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\n",
      "XGBoost saved at:\n",
      "C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\\xgboost_classifier.pkl\n",
      "Scaler saved at:\n",
      "C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\\feature_scaler.pkl\n",
      "\n",
      "Files currently in model directory:\n",
      "['best_pcb_model.pth', 'feature_scaler.pkl', 'hybrid_model_metadata.json', 'xgboost_classifier.pkl']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_DIR = Path(\n",
    "    r\"C:\\Users\\TUSHAR\\2025-26\\PROJECTS\\pcb_defect_detection\\models\"\n",
    ")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving to directory:\")\n",
    "print(MODEL_DIR.resolve())\n",
    "\n",
    "# Save XGBoost model\n",
    "xgb_path = MODEL_DIR / \"xgboost_classifier.pkl\"\n",
    "joblib.dump(xgb_model, xgb_path)\n",
    "\n",
    "print(\"XGBoost saved at:\")\n",
    "print(xgb_path.resolve())\n",
    "\n",
    "# Save scaler (if used)\n",
    "scaler_path = MODEL_DIR / \"feature_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(\"Scaler saved at:\")\n",
    "print(scaler_path.resolve())\n",
    "\n",
    "print(\"\\nFiles currently in model directory:\")\n",
    "print(os.listdir(MODEL_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1978a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
